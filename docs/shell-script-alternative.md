# Shell Script: ì§„ì§œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ì˜ ì™•ë„

## ì„œë¡ 

cli-pipeê°€ ìˆ˜ë°± ì¤„ì˜ Go ì½”ë“œë¡œ êµ¬í˜„í•˜ë ¤ëŠ” ëª¨ë“  ê²ƒì„ Shell Scriptë¡œ ë” ê°„ë‹¨í•˜ê³  ê°•ë ¥í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì´ ë¬¸ì„œëŠ” "ê¸°ë˜¥ì°¬ Shell Script"ë¡œ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.

## 1. Shell Scriptì˜ ìˆ¨ê²¨ì§„ ê°•ë ¥í•¨

### ê¸°ë³¸ í…œí”Œë¦¿

```bash
#!/usr/bin/env bash
# pipeline.sh - ì´ê²Œ ì „ë¶€ë‹¤

set -euo pipefail  # ì—„ê²©í•œ ì—ëŸ¬ ì²˜ë¦¬
IFS=$'\n\t'       # ì•ˆì „í•œ í•„ë“œ êµ¬ë¶„

# ìƒ‰ìƒ ì¶œë ¥
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m'

# ë¡œê¹… í•¨ìˆ˜
log() { echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $*"; }
error() { echo -e "${RED}[ERROR]${NC} $*" >&2; }
warn() { echo -e "${YELLOW}[WARN]${NC} $*" >&2; }
info() { echo -e "${BLUE}[INFO]${NC} $*"; }

# ì§„í–‰ë¥  í‘œì‹œ
progress() {
    local current=$1
    local total=$2
    local width=50
    local percentage=$((current * 100 / total))
    local completed=$((width * current / total))
    
    printf "\r${BLUE}["
    printf "%${completed}s" | tr ' ' '='
    printf "%$((width - completed))s" | tr ' ' ' '
    printf "] %d%%${NC}" "$percentage"
}

# ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
time_it() {
    local start=$(date +%s)
    "$@"
    local end=$(date +%s)
    log "â±ï¸  Execution time: $((end - start)) seconds"
}

# ì •ë¦¬ í•¨ìˆ˜
cleanup() {
    log "ğŸ§¹ Cleaning up..."
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    rm -f /tmp/pipeline_*
    # ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    jobs -p | xargs -r kill 2>/dev/null
}
trap cleanup EXIT
```

### cli-pipe vs Shell Script ë¹„êµ

**cli-pipe ë°©ì‹:**
```yaml
# pipeline.yaml
name: backup-pipeline
steps:
  - name: extract
    run: tar cf - /data
    output: data.tar
  - name: compress
    run: gzip -9
    input: data.tar
  - name: checksum
    run: sha256sum
    input: data.tar
```

**Shell Script ë°©ì‹:**
```bash
#!/usr/bin/env bash
# backup-pipeline.sh

log "ğŸš€ Starting backup pipeline"

# ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸
tar cf - /data | tee >(sha256sum > backup.sha256) | gzip -9 > backup.tar.gz

log "âœ… Backup completed"
log "ğŸ“¦ Archive: backup.tar.gz"
log "ğŸ” Checksum: backup.sha256"
```

## 2. ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ì˜ ì§„ì§œ êµ¬í˜„

### ëª…ëª…ëœ íŒŒì´í”„ë¥¼ ì´ìš©í•œ ë¶„ê¸°

```bash
#!/usr/bin/env bash
# streaming-pipeline.sh

# ëª…ëª…ëœ íŒŒì´í”„ ìƒì„±
mkfifo /tmp/pipe_compress /tmp/pipe_checksum /tmp/pipe_analyze

# ë°±ê·¸ë¼ìš´ë“œ ì†Œë¹„ìë“¤
{
    log "ğŸ—œï¸  Starting compression..."
    gzip -9 < /tmp/pipe_compress > backup.tar.gz
    log "âœ… Compression completed"
} &

{
    log "ğŸ” Calculating checksum..."
    sha256sum < /tmp/pipe_checksum > backup.sha256
    log "âœ… Checksum completed"
} &

{
    log "ğŸ“Š Analyzing data..."
    wc -l < /tmp/pipe_analyze > analysis.txt
    log "âœ… Analysis completed"
} &

# ìƒì‚°ìê°€ ëª¨ë“  ì†Œë¹„ìì—ê²Œ ë°ì´í„° ì „ì†¡
log "ğŸ“¤ Extracting data..."
tar cf - /data | tee /tmp/pipe_compress /tmp/pipe_checksum > /tmp/pipe_analyze

# ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
wait
log "ğŸ‰ All pipeline tasks completed!"

# ì •ë¦¬
rm -f /tmp/pipe_*
```

### ë™ì  íŒŒì´í”„ë¼ì¸ êµ¬ì„±

```bash
#!/usr/bin/env bash
# dynamic-pipeline.sh

# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì •ì˜
declare -a PIPELINE_STEPS=(
    "extract:tar cf - /data"
    "compress:gzip -9"
    "encrypt:openssl enc -aes-256-cbc -pass pass:secret"
    "upload:aws s3 cp - s3://backup-bucket/$(date +%Y%m%d).tar.gz.enc"
)

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
execute_pipeline() {
    local cmd_chain=""
    
    for step in "${PIPELINE_STEPS[@]}"; do
        local name="${step%%:*}"
        local command="${step#*:}"
        
        if [[ -z "$cmd_chain" ]]; then
            cmd_chain="$command"
        else
            cmd_chain="$cmd_chain | $command"
        fi
        
        log "ğŸ“‹ Added step: $name"
    done
    
    log "ğŸš€ Executing pipeline: $cmd_chain"
    eval "$cmd_chain"
}

execute_pipeline
```

## 3. ë³‘ë ¬ ì‹¤í–‰ê³¼ ì˜ì¡´ì„± ê´€ë¦¬

### ê³ ê¸‰ ì˜ì¡´ì„± ê´€ë¦¬

```bash
#!/usr/bin/env bash
# dependency-pipeline.sh

# ì‘ì—… ì •ì˜
declare -A JOBS=(
    [download_data]="curl -L https://example.com/data.tar.gz -o data.tar.gz"
    [download_config]="curl -L https://example.com/config.json -o config.json"
    [extract]="tar xf data.tar.gz"
    [validate]="python validate.py config.json"
    [process]="python process.py --config config.json"
    [backup]="cp processed_data.json backup/"
    [upload]="aws s3 cp processed_data.json s3://results/"
)

# ì˜ì¡´ì„± ì •ì˜
declare -A DEPENDENCIES=(
    [download_data]=""
    [download_config]=""
    [extract]="download_data"
    [validate]="download_config"
    [process]="extract validate"
    [backup]="process"
    [upload]="process"
)

# ì‘ì—… ìƒíƒœ ì¶”ì 
declare -A JOB_STATUS=()
declare -A JOB_PIDS=()

# ì‘ì—… ì‹¤í–‰ í•¨ìˆ˜
execute_job() {
    local job_name="$1"
    local job_cmd="${JOBS[$job_name]}"
    
    log "ğŸš€ Starting job: $job_name"
    JOB_STATUS[$job_name]="running"
    
    # ì‹¤ì œ ëª…ë ¹ì–´ ì‹¤í–‰
    if eval "$job_cmd"; then
        JOB_STATUS[$job_name]="completed"
        log "âœ… Job completed: $job_name"
    else
        JOB_STATUS[$job_name]="failed"
        error "âŒ Job failed: $job_name"
        return 1
    fi
}

# ì˜ì¡´ì„± í™•ì¸
check_dependencies() {
    local job_name="$1"
    local deps="${DEPENDENCIES[$job_name]}"
    
    if [[ -z "$deps" ]]; then
        return 0  # ì˜ì¡´ì„± ì—†ìŒ
    fi
    
    for dep in $deps; do
        if [[ "${JOB_STATUS[$dep]}" != "completed" ]]; then
            return 1  # ì˜ì¡´ì„± ë¯¸ì™„ë£Œ
        fi
    done
    
    return 0  # ëª¨ë“  ì˜ì¡´ì„± ì™„ë£Œ
}

# ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… ì°¾ê¸°
find_ready_jobs() {
    for job in "${!JOBS[@]}"; do
        if [[ "${JOB_STATUS[$job]}" == "" ]] && check_dependencies "$job"; then
            echo "$job"
        fi
    done
}

# ë©”ì¸ ì‹¤í–‰ ë£¨í”„
main() {
    local max_parallel=${MAX_PARALLEL:-4}
    local running_jobs=0
    
    # ëª¨ë“  ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
    for job in "${!JOBS[@]}"; do
        JOB_STATUS[$job]=""
    done
    
    while true; do
        # ì™„ë£Œëœ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í™•ì¸
        for job in "${!JOB_PIDS[@]}"; do
            if ! kill -0 "${JOB_PIDS[$job]}" 2>/dev/null; then
                unset JOB_PIDS[$job]
                ((running_jobs--))
            fi
        done
        
        # ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… ì°¾ê¸°
        if [[ $running_jobs -lt $max_parallel ]]; then
            local ready_jobs=($(find_ready_jobs))
            
            for job in "${ready_jobs[@]}"; do
                if [[ $running_jobs -lt $max_parallel ]]; then
                    execute_job "$job" &
                    JOB_PIDS[$job]=$!
                    ((running_jobs++))
                fi
            done
        fi
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œ í™•ì¸
        local all_done=true
        for job in "${!JOBS[@]}"; do
            if [[ "${JOB_STATUS[$job]}" != "completed" ]]; then
                all_done=false
                break
            fi
        done
        
        if [[ "$all_done" == true ]]; then
            break
        fi
        
        sleep 1
    done
    
    log "ğŸ‰ All jobs completed!"
}

main
```

## 4. ì—ëŸ¬ ì²˜ë¦¬ì™€ ì¬ì‹œë„

### ë˜‘ë˜‘í•œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

```bash
#!/usr/bin/env bash
# retry-pipeline.sh

# ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
retry_with_backoff() {
    local max_attempts="${1:-3}"
    local base_delay="${2:-1}"
    local max_delay="${3:-60}"
    shift 3
    
    local attempt=1
    local delay=$base_delay
    
    while (( attempt <= max_attempts )); do
        log "ğŸ”„ Attempt $attempt/$max_attempts: $*"
        
        if "$@"; then
            log "âœ… Command succeeded: $*"
            return 0
        fi
        
        if (( attempt == max_attempts )); then
            error "âŒ Command failed after $max_attempts attempts: $*"
            return 1
        fi
        
        warn "â³ Retrying in ${delay}s..."
        sleep "$delay"
        
        # ì§€ìˆ˜ ë°±ì˜¤í”„ (ìµœëŒ€ delayê¹Œì§€)
        delay=$((delay * 2))
        if (( delay > max_delay )); then
            delay=$max_delay
        fi
        
        ((attempt++))
    done
}

# ì¡°ê±´ë¶€ ì¬ì‹œë„
retry_on_condition() {
    local condition_check="$1"
    local max_attempts="${2:-5}"
    shift 2
    
    local attempt=1
    
    while (( attempt <= max_attempts )); do
        if "$@"; then
            return 0
        fi
        
        # ì¡°ê±´ í™•ì¸
        if ! eval "$condition_check"; then
            error "âŒ Condition not met, giving up: $condition_check"
            return 1
        fi
        
        warn "ğŸ”„ Condition met, retrying ($attempt/$max_attempts)..."
        sleep $((attempt * 2))
        ((attempt++))
    done
    
    return 1
}

# ì‚¬ìš© ì˜ˆì œ
demo_retry() {
    log "ğŸŒ Testing network operations..."
    
    # ë„¤íŠ¸ì›Œí¬ ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„
    retry_with_backoff 5 2 30 \
        curl -f -L https://example.com/large-file.tar.gz -o data.tar.gz
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¬ì‹œë„
    retry_on_condition "ping -c 1 db.example.com >/dev/null" 10 \
        psql -h db.example.com -c "SELECT 1"
    
    log "âœ… All retry operations completed"
}
```

### ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´

```bash
#!/usr/bin/env bash
# circuit-breaker.sh

# ì„œí‚· ë¸Œë ˆì´ì»¤ ìƒíƒœ
declare -A CIRCUIT_STATE=()
declare -A CIRCUIT_FAILURES=()
declare -A CIRCUIT_LAST_FAILURE=()

# ì„œí‚· ë¸Œë ˆì´ì»¤ ì„¤ì •
readonly FAILURE_THRESHOLD=5
readonly RECOVERY_TIMEOUT=60

# ì„œí‚· ë¸Œë ˆì´ì»¤ ì‹¤í–‰
circuit_breaker() {
    local circuit_name="$1"
    shift
    
    local state="${CIRCUIT_STATE[$circuit_name]:-closed}"
    local failures="${CIRCUIT_FAILURES[$circuit_name]:-0}"
    local last_failure="${CIRCUIT_LAST_FAILURE[$circuit_name]:-0}"
    local now=$(date +%s)
    
    case "$state" in
        open)
            if (( now - last_failure > RECOVERY_TIMEOUT )); then
                log "ğŸ”„ Circuit $circuit_name: attempting recovery"
                CIRCUIT_STATE[$circuit_name]="half-open"
            else
                error "âš¡ Circuit $circuit_name: open (failing fast)"
                return 1
            fi
            ;;
        half-open)
            log "ğŸ” Circuit $circuit_name: testing in half-open state"
            ;;
        closed)
            log "âœ… Circuit $circuit_name: closed (normal operation)"
            ;;
    esac
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if "$@"; then
        # ì„±ê³µì‹œ ì„œí‚· ë³µêµ¬
        CIRCUIT_STATE[$circuit_name]="closed"
        CIRCUIT_FAILURES[$circuit_name]=0
        log "âœ… Circuit $circuit_name: command succeeded"
        return 0
    else
        # ì‹¤íŒ¨ì‹œ ì„œí‚· ìƒíƒœ ì—…ë°ì´íŠ¸
        ((failures++))
        CIRCUIT_FAILURES[$circuit_name]=$failures
        CIRCUIT_LAST_FAILURE[$circuit_name]=$now
        
        if (( failures >= FAILURE_THRESHOLD )); then
            CIRCUIT_STATE[$circuit_name]="open"
            error "âš¡ Circuit $circuit_name: opened due to failures ($failures)"
        fi
        
        error "âŒ Circuit $circuit_name: command failed"
        return 1
    fi
}
```

## 5. ì„¤ì •ê³¼ íŒŒë¼ë¯¸í„° ê´€ë¦¬

### í™˜ê²½ ê¸°ë°˜ ì„¤ì •

```bash
#!/usr/bin/env bash
# config-pipeline.sh

# ê¸°ë³¸ ì„¤ì •
declare -A DEFAULT_CONFIG=(
    [SOURCE_DIR]="/data"
    [BACKUP_DIR]="/backup"
    [COMPRESSION_LEVEL]="9"
    [PARALLEL_JOBS]="$(nproc)"
    [RETENTION_DAYS]="30"
    [LOG_LEVEL]="INFO"
    [DRY_RUN]="false"
)

# ì„¤ì • íŒŒì¼ ë¡œë“œ
load_config() {
    local config_file="${1:-pipeline.conf}"
    
    if [[ -f "$config_file" ]]; then
        log "ğŸ“‹ Loading config from: $config_file"
        # shellcheck source=/dev/null
        source "$config_file"
    fi
}

# í™˜ê²½ ë³€ìˆ˜ ì ìš©
apply_env_config() {
    for key in "${!DEFAULT_CONFIG[@]}"; do
        if [[ -n "${!key:-}" ]]; then
            log "ğŸ”§ Using environment variable: $key=${!key}"
        else
            declare -g "$key"="${DEFAULT_CONFIG[$key]}"
            log "ğŸ“Œ Using default value: $key=${DEFAULT_CONFIG[$key]}"
        fi
    done
}

# ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -s|--source)
                SOURCE_DIR="$2"
                shift 2
                ;;
            -d|--dest)
                BACKUP_DIR="$2"
                shift 2
                ;;
            -j|--jobs)
                PARALLEL_JOBS="$2"
                shift 2
                ;;
            -c|--compression)
                COMPRESSION_LEVEL="$2"
                shift 2
                ;;
            --dry-run)
                DRY_RUN="true"
                shift
                ;;
            -v|--verbose)
                LOG_LEVEL="DEBUG"
                shift
                ;;
            -h|--help)
                show_help
                exit 0
                ;;
            *)
                error "Unknown option: $1"
                show_help
                exit 1
                ;;
        esac
    done
}

# ì„¤ì • ê²€ì¦
validate_config() {
    local errors=0
    
    # ë””ë ‰í† ë¦¬ ê²€ì¦
    if [[ ! -d "$SOURCE_DIR" ]]; then
        error "Source directory does not exist: $SOURCE_DIR"
        ((errors++))
    fi
    
    # ìˆ«ì ê²€ì¦
    if ! [[ "$COMPRESSION_LEVEL" =~ ^[1-9]$ ]]; then
        error "Invalid compression level: $COMPRESSION_LEVEL (must be 1-9)"
        ((errors++))
    fi
    
    if ! [[ "$PARALLEL_JOBS" =~ ^[1-9][0-9]*$ ]]; then
        error "Invalid parallel jobs: $PARALLEL_JOBS (must be positive integer)"
        ((errors++))
    fi
    
    return $errors
}

# ë„ì›€ë§ í‘œì‹œ
show_help() {
    cat << 'EOF'
Usage: pipeline.sh [OPTIONS]

Options:
  -s, --source DIR      Source directory (default: /data)
  -d, --dest DIR        Backup directory (default: /backup)
  -j, --jobs N          Parallel jobs (default: CPU cores)
  -c, --compression N   Compression level 1-9 (default: 9)
      --dry-run         Show what would be done
  -v, --verbose         Enable verbose logging
  -h, --help            Show this help

Environment variables:
  SOURCE_DIR, BACKUP_DIR, COMPRESSION_LEVEL, PARALLEL_JOBS,
  RETENTION_DAYS, LOG_LEVEL, DRY_RUN

Configuration file:
  pipeline.conf (key=value format)
EOF
}

# ë©”ì¸ ì„¤ì • ì´ˆê¸°í™”
init_config() {
    load_config "$@"
    parse_args "$@"
    apply_env_config
    validate_config
}
```

## 6. ëª¨ë‹ˆí„°ë§ê³¼ ë¡œê¹…

### êµ¬ì¡°í™”ëœ ë¡œê¹…

```bash
#!/usr/bin/env bash
# logging-pipeline.sh

# ë¡œê·¸ ë ˆë²¨ ì •ì˜
declare -A LOG_LEVELS=(
    [DEBUG]=0
    [INFO]=1
    [WARN]=2
    [ERROR]=3
)

# í˜„ì¬ ë¡œê·¸ ë ˆë²¨
CURRENT_LOG_LEVEL=${LOG_LEVEL:-INFO}

# ë¡œê·¸ í•¨ìˆ˜
log_with_level() {
    local level="$1"
    local message="$2"
    shift 2
    
    local level_num=${LOG_LEVELS[$level]}
    local current_num=${LOG_LEVELS[$CURRENT_LOG_LEVEL]}
    
    if (( level_num >= current_num )); then
        local timestamp=$(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")
        local color
        
        case "$level" in
            DEBUG) color=$'\033[0;37m' ;;  # White
            INFO)  color=$'\033[0;32m' ;;  # Green
            WARN)  color=$'\033[1;33m' ;;  # Yellow
            ERROR) color=$'\033[0;31m' ;;  # Red
        esac
        
        printf "%s[%s] %s%s %s\n" \
            "$color" "$timestamp" "$level" "$NC" "$message" "$@"
    fi
}

debug() { log_with_level DEBUG "$@"; }
info() { log_with_level INFO "$@"; }
warn() { log_with_level WARN "$@"; }
error() { log_with_level ERROR "$@"; }

# JSON ë¡œê¹…
log_json() {
    local level="$1"
    local message="$2"
    local metadata="${3:-{}}"
    
    jq -n \
        --arg timestamp "$(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")" \
        --arg level "$level" \
        --arg message "$message" \
        --argjson metadata "$metadata" \
        --arg hostname "$(hostname)" \
        --arg pid "$$" \
        '{
            timestamp: $timestamp,
            level: $level,
            message: $message,
            metadata: $metadata,
            hostname: $hostname,
            pid: $pid
        }'
}

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘
declare -A METRICS=()

metric() {
    local name="$1"
    local value="$2"
    local timestamp=$(date +%s)
    
    METRICS["$name"]="$value"
    
    # ë©”íŠ¸ë¦­ ë¡œê¹…
    log_json "METRIC" "Recorded metric: $name" \
        "$(jq -n --arg name "$name" --arg value "$value" --arg timestamp "$timestamp" \
            '{name: $name, value: $value, timestamp: $timestamp}')"
}

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
monitor_performance() {
    local interval=${1:-5}
    local logfile="${2:-/tmp/pipeline_metrics.log}"
    
    while true; do
        local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
        local memory_usage=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
        local disk_usage=$(df /tmp | tail -1 | awk '{print $5}' | cut -d'%' -f1)
        
        {
            echo "timestamp=$(date +%s)"
            echo "cpu_usage=$cpu_usage"
            echo "memory_usage=$memory_usage"
            echo "disk_usage=$disk_usage"
            echo "---"
        } >> "$logfile"
        
        sleep "$interval"
    done &
    
    echo $!  # ëª¨ë‹ˆí„°ë§ í”„ë¡œì„¸ìŠ¤ PID ë°˜í™˜
}
```

### ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ

```bash
#!/usr/bin/env bash
# dashboard-pipeline.sh

# ëŒ€ì‹œë³´ë“œ í‘œì‹œ
show_dashboard() {
    local log_file="$1"
    
    while true; do
        clear
        
        # í—¤ë”
        echo -e "${BLUE}================================${NC}"
        echo -e "${BLUE}    Pipeline Dashboard${NC}"
        echo -e "${BLUE}================================${NC}"
        echo
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        echo -e "${GREEN}System Status:${NC}"
        echo "  Time: $(date)"
        echo "  Load: $(uptime | awk -F'load average:' '{print $2}')"
        echo "  Memory: $(free -h | grep Mem | awk '{print $3"/"$2}')"
        echo
        
        # ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…
        echo -e "${GREEN}Running Jobs:${NC}"
        jobs -l | while read -r job; do
            echo "  $job"
        done
        echo
        
        # ìµœê·¼ ë¡œê·¸
        echo -e "${GREEN}Recent Logs:${NC}"
        if [[ -f "$log_file" ]]; then
            tail -10 "$log_file"
        fi
        
        sleep 2
    done
}

# ì§„í–‰ë¥  í‘œì‹œ
show_progress_bar() {
    local current="$1"
    local total="$2"
    local width="${3:-50}"
    local label="${4:-Progress}"
    
    local percentage=$((current * 100 / total))
    local filled=$((width * current / total))
    local empty=$((width - filled))
    
    printf "\r%s: [" "$label"
    printf "%*s" "$filled" | tr ' ' '='
    printf "%*s" "$empty" | tr ' ' '-'
    printf "] %d%% (%d/%d)" "$percentage" "$current" "$total"
}

# ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥
tail_logs() {
    local log_file="$1"
    local pattern="${2:-.*}"
    
    tail -f "$log_file" | while read -r line; do
        if [[ "$line" =~ $pattern ]]; then
            case "$line" in
                *ERROR*) echo -e "${RED}$line${NC}" ;;
                *WARN*)  echo -e "${YELLOW}$line${NC}" ;;
                *INFO*)  echo -e "${GREEN}$line${NC}" ;;
                *)       echo "$line" ;;
            esac
        fi
    done
}
```

## 7. ì‹¤ì œ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ì˜ˆì œ

### ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```bash
#!/usr/bin/env bash
# complete-pipeline.sh

# ì„¤ì • ì´ˆê¸°í™”
source "$(dirname "$0")/lib/config.sh"
source "$(dirname "$0")/lib/logging.sh"
source "$(dirname "$0")/lib/retry.sh"

# íŒŒì´í”„ë¼ì¸ ì •ì˜
declare -A PIPELINE_STEPS=(
    [download]="download_data"
    [validate]="validate_data"
    [transform]="transform_data"
    [analyze]="analyze_data"
    [report]="generate_report"
    [cleanup]="cleanup_temp_files"
)

declare -A STEP_DEPENDENCIES=(
    [download]=""
    [validate]="download"
    [transform]="validate"
    [analyze]="transform"
    [report]="analyze"
    [cleanup]="report"
)

# ë‹¨ê³„ë³„ êµ¬í˜„
download_data() {
    info "ğŸ“¥ Downloading data..."
    
    local urls=(
        "https://example.com/data1.csv"
        "https://example.com/data2.json"
        "https://example.com/data3.xml"
    )
    
    local pids=()
    for url in "${urls[@]}"; do
        local filename=$(basename "$url")
        retry_with_backoff 3 2 30 \
            curl -L "$url" -o "data/$filename" &
        pids+=($!)
    done
    
    # ëª¨ë“  ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
    for pid in "${pids[@]}"; do
        wait "$pid" || return 1
    done
    
    info "âœ… Download completed"
}

validate_data() {
    info "âœ… Validating data..."
    
    local errors=0
    
    # CSV ê²€ì¦
    if ! head -1 data/data1.csv | grep -q "id,name,value"; then
        error "Invalid CSV header"
        ((errors++))
    fi
    
    # JSON ê²€ì¦
    if ! jq empty data/data2.json; then
        error "Invalid JSON format"
        ((errors++))
    fi
    
    # XML ê²€ì¦
    if ! xmllint --noout data/data3.xml; then
        error "Invalid XML format"
        ((errors++))
    fi
    
    if (( errors > 0 )); then
        error "Data validation failed with $errors errors"
        return 1
    fi
    
    info "âœ… Data validation passed"
}

transform_data() {
    info "ğŸ”„ Transforming data..."
    
    # CSV to JSON
    {
        echo "Converting CSV to JSON..."
        csv2json data/data1.csv > processed/data1.json
    } &
    
    # JSON normalization
    {
        echo "Normalizing JSON..."
        jq '.[] | {id: .id, name: .name, value: (.value | tonumber)}' \
            data/data2.json > processed/data2_normalized.json
    } &
    
    # XML to JSON
    {
        echo "Converting XML to JSON..."
        xmlstarlet sel -t -o '{"items":[' \
            -m "//item" -o '{"id":"' -v "@id" -o '","name":"' -v "name" -o '"},' \
            -o ']}' data/data3.xml | sed 's/,]/]}/' > processed/data3.json
    } &
    
    wait  # ëª¨ë“  ë³€í™˜ ì™„ë£Œ ëŒ€ê¸°
    
    info "âœ… Data transformation completed"
}

analyze_data() {
    info "ğŸ“Š Analyzing data..."
    
    # ë°ì´í„° í†µê³„
    {
        echo "Calculating statistics..."
        jq -s '
            map(select(type == "array") | .[]) |
            group_by(.name) |
            map({
                name: .[0].name,
                count: length,
                avg_value: (map(.value) | add / length)
            })
        ' processed/*.json > analysis/statistics.json
    } &
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
    {
        echo "Checking data quality..."
        jq -s '
            map(select(type == "array") | .[]) |
            {
                total_records: length,
                null_values: map(select(.value == null)) | length,
                duplicate_ids: (group_by(.id) | map(select(length > 1)) | length)
            }
        ' processed/*.json > analysis/quality.json
    } &
    
    wait
    
    info "âœ… Data analysis completed"
}

generate_report() {
    info "ğŸ“‹ Generating report..."
    
    # HTML ë¦¬í¬íŠ¸ ìƒì„±
    cat > reports/report.html << 'EOF'
<!DOCTYPE html>
<html>
<head>
    <title>Pipeline Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .metric { background: #f0f0f0; padding: 10px; margin: 10px 0; }
        .success { color: green; }
        .error { color: red; }
    </style>
</head>
<body>
    <h1>Pipeline Execution Report</h1>
    <div class="metric">
        <h2>Execution Time</h2>
        <p>Started: $(date)</p>
        <p>Duration: ${SECONDS} seconds</p>
    </div>
    
    <div class="metric">
        <h2>Data Statistics</h2>
        <pre>$(cat analysis/statistics.json | jq .)</pre>
    </div>
    
    <div class="metric">
        <h2>Data Quality</h2>
        <pre>$(cat analysis/quality.json | jq .)</pre>
    </div>
</body>
</html>
EOF
    
    # ì´ë©”ì¼ ì•Œë¦¼
    if command -v mail >/dev/null; then
        {
            echo "Pipeline execution completed successfully!"
            echo "Report: file://$(pwd)/reports/report.html"
            echo "Duration: ${SECONDS} seconds"
        } | mail -s "Pipeline Report" admin@example.com
    fi
    
    info "âœ… Report generated"
}

cleanup_temp_files() {
    info "ğŸ§¹ Cleaning up temporary files..."
    
    # 7ì¼ ì´ìƒëœ ì„ì‹œ íŒŒì¼ ì‚­ì œ
    find temp/ -type f -mtime +7 -delete
    
    # ë¡œê·¸ íŒŒì¼ ì••ì¶•
    find logs/ -name "*.log" -mtime +1 -exec gzip {} \;
    
    info "âœ… Cleanup completed"
}

# ë©”ì¸ ì‹¤í–‰
main() {
    info "ğŸš€ Starting complete pipeline..."
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p {data,processed,analysis,reports,temp,logs}
    
    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    local monitor_pid
    monitor_pid=$(monitor_performance 5 "logs/performance.log")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    for step in download validate transform analyze report cleanup; do
        info "â–¶ï¸  Executing step: $step"
        
        if time_it "${PIPELINE_STEPS[$step]}"; then
            info "âœ… Step completed: $step"
        else
            error "âŒ Step failed: $step"
            kill "$monitor_pid" 2>/dev/null
            return 1
        fi
    done
    
    # ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
    kill "$monitor_pid" 2>/dev/null
    
    info "ğŸ‰ Pipeline completed successfully!"
    info "ğŸ“Š Total execution time: ${SECONDS} seconds"
}

# ì‹¤í–‰
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
```

## 8. Shell Script í”„ë ˆì„ì›Œí¬

### bashpipe - íŒŒì´í”„ë¼ì¸ í”„ë ˆì„ì›Œí¬

```bash
#!/usr/bin/env bash
# bashpipe.sh - Shell Script Pipeline Framework

# í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
bashpipe::init() {
    set -euo pipefail
    
    # ì „ì—­ ë³€ìˆ˜
    declare -g -A BASHPIPE_STEPS=()
    declare -g -A BASHPIPE_DEPS=()
    declare -g -A BASHPIPE_CONFIG=()
    declare -g BASHPIPE_LOGFILE="/tmp/bashpipe.log"
    
    # ê¸°ë³¸ ì„¤ì •
    BASHPIPE_CONFIG[MAX_PARALLEL]=4
    BASHPIPE_CONFIG[LOG_LEVEL]="INFO"
    BASHPIPE_CONFIG[RETRY_COUNT]=3
    
    # ë¡œê¹… ì„¤ì •
    exec 1> >(tee -a "$BASHPIPE_LOGFILE")
    exec 2> >(tee -a "$BASHPIPE_LOGFILE" >&2)
}

# íŒŒì´í”„ë¼ì¸ ì •ì˜
bashpipe::pipeline() {
    local name="$1"
    shift
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --description)
                BASHPIPE_CONFIG[DESCRIPTION]="$2"
                shift 2
                ;;
            --on-error)
                BASHPIPE_CONFIG[ON_ERROR]="$2"
                shift 2
                ;;
            --parallel)
                BASHPIPE_CONFIG[MAX_PARALLEL]="$2"
                shift 2
                ;;
            *)
                echo "Unknown option: $1"
                return 1
                ;;
        esac
    done
    
    BASHPIPE_CONFIG[NAME]="$name"
    echo "ğŸ“‹ Pipeline defined: $name"
}

# ë‹¨ê³„ ì •ì˜
bashpipe::step() {
    local step_name="$1"
    shift
    
    local cmd=""
    local input=""
    local output=""
    local deps=""
    local parallel=1
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --run)
                cmd="$2"
                shift 2
                ;;
            --input)
                input="$2"
                shift 2
                ;;
            --output)
                output="$2"
                shift 2
                ;;
            --depends)
                deps="$2"
                shift 2
                ;;
            --parallel)
                parallel="$2"
                shift 2
                ;;
            *)
                echo "Unknown option: $1"
                return 1
                ;;
        esac
    done
    
    BASHPIPE_STEPS["$step_name"]="$cmd"
    BASHPIPE_DEPS["$step_name"]="$deps"
    
    echo "ğŸ“ Step defined: $step_name"
}

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
bashpipe::execute() {
    local pipeline_name="$1"
    
    echo "ğŸš€ Executing pipeline: $pipeline_name"
    echo "ğŸ“– Description: ${BASHPIPE_CONFIG[DESCRIPTION]:-No description}"
    
    # ì˜ì¡´ì„± ìˆœì„œë¡œ ì‹¤í–‰
    local executed=()
    local total_steps=${#BASHPIPE_STEPS[@]}
    local current_step=0
    
    for step in "${!BASHPIPE_STEPS[@]}"; do
        if bashpipe::_can_execute "$step" "${executed[@]}"; then
            ((current_step++))
            
            echo "â–¶ï¸  Step $current_step/$total_steps: $step"
            
            if bashpipe::_execute_step "$step"; then
                executed+=("$step")
                echo "âœ… Step completed: $step"
            else
                echo "âŒ Step failed: $step"
                return 1
            fi
        fi
    done
    
    echo "ğŸ‰ Pipeline completed: $pipeline_name"
}

# ë‹¨ê³„ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
bashpipe::_can_execute() {
    local step="$1"
    shift
    local executed=("$@")
    
    local deps="${BASHPIPE_DEPS[$step]}"
    
    if [[ -z "$deps" ]]; then
        return 0
    fi
    
    for dep in $deps; do
        if ! printf '%s\n' "${executed[@]}" | grep -q "^$dep$"; then
            return 1
        fi
    done
    
    return 0
}

# ë‹¨ê³„ ì‹¤í–‰
bashpipe::_execute_step() {
    local step="$1"
    local cmd="${BASHPIPE_STEPS[$step]}"
    
    local start_time=$(date +%s)
    
    if eval "$cmd"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        echo "â±ï¸  Step '$step' completed in ${duration}s"
        return 0
    else
        echo "âŒ Step '$step' failed"
        return 1
    fi
}

# ì‚¬ìš© ì˜ˆì œ
demo_framework() {
    # í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
    bashpipe::init
    
    # íŒŒì´í”„ë¼ì¸ ì •ì˜
    bashpipe::pipeline "data-processing" \
        --description "Process data files" \
        --on-error "stop" \
        --parallel 2
    
    # ë‹¨ê³„ ì •ì˜
    bashpipe::step "download" \
        --run "curl -L https://example.com/data.csv -o data.csv"
    
    bashpipe::step "validate" \
        --run "python validate.py data.csv" \
        --depends "download"
    
    bashpipe::step "process" \
        --run "python process.py data.csv" \
        --depends "validate"
    
    bashpipe::step "backup" \
        --run "cp processed_data.json backup/" \
        --depends "process"
    
    # ì‹¤í–‰
    bashpipe::execute "data-processing"
}
```

## 9. ì„±ëŠ¥ ë¹„êµ

### Shell Script vs cli-pipe

**ì‹œì‘ ì‹œê°„:**
- Shell Script: ì¦‰ì‹œ (bash ë¡œë”© ì‹œê°„ë§Œ)
- cli-pipe: Go ë°”ì´ë„ˆë¦¬ ë¡œë”© + YAML íŒŒì‹± + ê²€ì¦

**ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:**
- Shell Script: ìµœì†Œ (bash í”„ë¡œì„¸ìŠ¤ë§Œ)
- cli-pipe: Go ëŸ°íƒ€ì„ + ì¶”ê°€ ë©”ëª¨ë¦¬ í• ë‹¹

**ì‹¤í–‰ ì˜¤ë²„í—¤ë“œ:**
- Shell Script: ì—†ìŒ (ë„¤ì´í‹°ë¸Œ Unix)
- cli-pipe: Go â†’ bash ë³€í™˜ + ì¶”ê°€ í”„ë¡œì„¸ìŠ¤

**ë””ë²„ê¹…:**
- Shell Script: `bash -x` í•œ ì¤„ë¡œ ëª¨ë“  ê²ƒ ì¶”ì 
- cli-pipe: Go ë””ë²„ê±° + ë¡œê·¸ ë¶„ì„

## 10. ê²°ë¡ 

### Shell Scriptê°€ ë‚˜ì€ ì´ìœ 

1. **ë‹¨ìˆœí•¨**: ì¶”ê°€ ëŸ°íƒ€ì„ ì—†ìŒ
2. **íˆ¬ëª…ì„±**: ëª¨ë“  ê²ƒì´ ë³´ì„
3. **ì„±ëŠ¥**: ë„¤ì´í‹°ë¸Œ Unix ì„±ëŠ¥
4. **ìœ ì—°ì„±**: ì‰˜ì˜ ëª¨ë“  ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥
5. **ë””ë²„ê¹…**: í‘œì¤€ ë„êµ¬ë¡œ ì™„ë²½ ì¶”ì 
6. **ì´ì‹ì„±**: ëª¨ë“  Unix ì‹œìŠ¤í…œì—ì„œ ë™ì‘

### cli-pipeê°€ ë‚˜ì€ ê²½ìš°

1. **íŒ€ í‘œì¤€í™”**: "YAMLì´ ë” ì½ê¸° ì‰¬ì›Œìš”"
2. **Go ìƒíƒœê³„**: ë‹¤ë¥¸ Go ë„êµ¬ì™€ í†µí•©
3. **íƒ€ì… ì•ˆì „ì„±**: ì»´íŒŒì¼ íƒ€ì„ ì²´í¬
4. **ì›¹ UI**: í–¥í›„ ì›¹ ì¸í„°í˜ì´ìŠ¤ ê³„íš

### ì§„ì§œ ê²°ë¡ 

> "ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ìˆœí•˜ê²Œ í•´ê²°í•˜ëŠ” ê²ƒì´ ì²œì¬ë‹¤." - ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸

cli-pipeê°€ ìˆ˜ë°± ì¤„ì˜ Go ì½”ë“œë¡œ êµ¬í˜„í•˜ë ¤ëŠ” ëª¨ë“  ê¸°ëŠ¥ì„ Shell Scriptë¡œ ë” ê°„ë‹¨í•˜ê³  ê°•ë ¥í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. 

**ì§„ì§œ Unix ì² í•™:**
- ì‘ì€ ë„êµ¬ë“¤ì„ ì¡°í•©í•˜ë¼
- í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•˜ë¼
- ì¹¨ë¬µì€ ê¸ˆì´ë‹¤
- ì‹¤íŒ¨ëŠ” í° ì†Œë¦¬ë¡œ í•˜ë¼

Shell ScriptëŠ” ì´ ëª¨ë“  ì›ì¹™ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë”°ë¥¸ë‹¤. cli-pipeëŠ” ì´ë¥¼ ì–µì§€ë¡œ ì¬êµ¬í˜„í•œë‹¤.

> "The best code is no code at all." - Jeff Atwood

í•„ìš” ì—†ëŠ” ì½”ë“œë¥¼ ë§Œë“¤ì§€ ë§ê³ , ì´ë¯¸ ì™„ë²½í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë¼.

---

*ì´ ë¬¸ì„œëŠ” Shell Scriptì˜ ì§„ì§œ ê°•ë ¥í•¨ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë•Œë¡œëŠ” ê°€ì¥ ì˜¤ë˜ëœ ë„êµ¬ê°€ ê°€ì¥ ì¢‹ì€ ë„êµ¬ì…ë‹ˆë‹¤.*